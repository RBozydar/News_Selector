---
title: "Metodologia"
---

Przy wyznaczaniu temtów uwzględniono tylko te słowa, które znalazły się wśród 3% wyrazów o największej ilości wystąpień w danym miesiącu oraz były kluczowe - tzn. ich statystyka Dunninga wyniosła co najmniej 50. Dokładna miara kluczowości statystycznej [Dunninga (1993)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962) została podana w pierwszej tabeli. Przy wyliczaniu tej miary, częstość występowania słowa w danym miesiącu porównywana jest z częstotliwością w okresie od 1 stycznia 2018 do 31 grudnia 2019. Wysoka wartość statystyki oznacza, że dane w danym miesiącu, słowo pojawiało się w artykułach częściej niż w okresie referencyjnym.

Dla wszystkich słów, które wystąpiły w danym miesiącu wyznaczono *embeddingi*, czyli wektorową reprezentację słów. Uzyskana je za pomocą dekompozycji (*SVD*) macierzy występowania słów (*TF matrix*) w akapitach i artykułach. W analizie tekstu, taki sposób redukcji wymiarów macierzy *TF* nazywany jest jako *Latent Semantic Analysis* (*LSA*). Procedura ta jest analogiczna do analizy głównych składowych (*PCA*), ale nie wymaga standaryzacji poszczególnych kolumn macierzy *TF*. Dzięki temu w algorytmie dekompozycji można wykorzystać macierze rzadkie, czyli takie, które przechowują jedynie niezerowe informacje. Redukcja wymiarów pozwala na zmniejszenie liczby kolumn z około 140 tysięcu do zaledwie 512, przy jednoczesnym zachowaniu informacji o współwystępowaniu poszczególnych słów. 

Poszczególne słowa zostały automatycznie pogrupowane w tematy na podstawie podobieństwa cosinusowego między ich embeddingami. W tym celu wykorzystano algromeracyjne grupowanie hierarchiczne. Optymalną liczbę tematów wyznaczono przy pomocy algorytmu [silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)). Embedding nowo powstałej grupy wyznaczano jako sumę embeddingów słów, które zajdują się w danej grupie.

Zdania, które podsumowują poszczególne tematy zostały wybrane za pomocą zmodyfikowanego algorytmu [LexRank](https://blog.nus.edu.sg/soctalent/2010/02/11/a-brief-summary-of-lexrank-graph-based-lexical-centrality-as-salience-in-text-summarization/), opisanego w [artykule](https://pdfs.semanticscholar.org/44fc/a068eecce2203d111213e3691647914a3945.pdf) z 2004 r. oraz za pomocą podobieństwa cosinusowego między *embeddingiem* zdania, a *embeddingiem* tematu.

Pełny opis metodologii znajduje się w [prezentacji](https://jkubajek.github.io/News_Selector/News_Selector.pdf).
